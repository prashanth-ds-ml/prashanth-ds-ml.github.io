---

title: Laying the Foundations ‚Äî Data Profiling & Quality Checks with SQL

---

# üìù Blog 1: Laying the Foundations ‚Äî Data Profiling & Quality Checks with SQL

Before diving into fancy dashboards and advanced analytics, every analyst‚Äôs first job is to **understand the data**. In this tutorial, we‚Äôll use SQL to profile the **Superstore sales dataset** and make sure it‚Äôs clean, consistent, and ready for analysis.

This is part of our 3-part learning series:

1. **Blog 1:** Data profiling & validation (you are here)
2. Blog 2: Aggregations & distributions for business insights
3. Blog 3: Time-series, customer lifetime, and advanced SQL

---

## 1. Connecting to the database

Always start by checking you‚Äôre in the right database:

```sql
USE super_store_sales;
```

Why? Because in real-world projects, you often have multiple databases (staging, test, prod). Using the wrong one could mean running queries on outdated or incomplete data.

---

## 2. Inspecting the table

Next, let‚Äôs peek into the **Sales** table:

```sql
DESCRIBE Sales;
```

This shows the schema: column names, data types, and whether they can hold NULL values.

To see what the data looks like:

```sql
SELECT * 
FROM Sales
LIMIT 10;
```

This gives the first 10 rows ‚Äî useful for sanity checks (e.g., are dates formatted correctly? are discounts decimals or percentages?).

---

## 3. Counting rows

Basic but important: how many records are there?

```sql
SELECT COUNT(*) AS total_rows
FROM Sales;
```

This tells you if the dataset matches expectations. For the Superstore dataset, you should see \~9,994 rows.

---

## 4. Checking the time span

Sales data should have a clear time range:

```sql
SELECT MIN(order_date) AS start_date,
       MAX(order_date) AS end_date
FROM Sales;
```

This confirms the dataset runs from **2014-01-03 to 2017-12-30**. Knowing the coverage is key for seasonal or year-over-year analysis.

---

## 5. Null checks

Messy datasets often have missing values. To count NULLs per column:

```sql
SELECT 
    SUM(CASE WHEN order_date IS NULL THEN 1 ELSE 0 END) AS null_order_date,
    SUM(CASE WHEN ship_date IS NULL THEN 1 ELSE 0 END)  AS null_ship_date,
    SUM(CASE WHEN sales IS NULL THEN 1 ELSE 0 END)      AS null_sales,
    SUM(CASE WHEN profit IS NULL THEN 1 ELSE 0 END)     AS null_profit
FROM Sales;
```

 If you see unexpected NULLs in critical fields like `order_date` or `sales`, that‚Äôs a red flag for data quality.

---

## 6. Profiling categorical columns

Categorical columns like **Segment**, **Region**, **Ship Mode**, or **Category** often define the dimensions for analysis.

To see all distinct values:

```sql
SELECT DISTINCT segment FROM Sales;
SELECT DISTINCT region FROM Sales;
SELECT DISTINCT ship_mode FROM Sales;
SELECT DISTINCT category FROM Sales;
```

Result:

* Segments ‚Üí Consumer, Corporate, Home Office
* Regions ‚Üí Central, East, South, West
* Categories ‚Üí Furniture, Office Supplies, Technology

Knowing these values upfront ensures you don‚Äôt miss categories when aggregating later.

---

## 7. Profiling numerical columns

Which columns are numeric? Let‚Äôs use aggregation:

```sql
SELECT 
    MIN(sales) AS min_sales,
    MAX(sales) AS max_sales,
    AVG(sales) AS avg_sales,
    STDDEV(sales) AS std_sales
FROM Sales;
```

You can repeat for `quantity`, `discount`, and `profit`.

 This gives a sense of scale (e.g., max sale is \$22,638, average is \$229). Spotting outliers early helps avoid misleading visualizations.

---

##  Key takeaways

* Always start with **profiling & validation** before analysis.
* Use `DESCRIBE`, `COUNT`, `MIN/MAX`, and NULL checks to ensure the dataset is healthy.
* Profile **categorical columns** (distinct values) and **numeric columns** (min, max, avg, stddev) to understand ranges and distributions.
* Knowing your dataset‚Äôs **time coverage** is crucial for time-based analysis later.

